{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "g_frame = 0\n",
    "cv2.namedWindow('image')\n",
    "cap = cv2.VideoCapture(0)  #打开摄像头\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) #获取视频每秒的帧数\n",
    "print(fps)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') # 视频的编码\n",
    "out = cv2.VideoWriter(\"output.avi\", fourcc, fps, (800, 600))# 定义视频输出\n",
    "strInfo = ''\n",
    "while (cap.isOpened()):\n",
    "#   第一个参数ret 为True 或者False,代表有没有读取到图片\n",
    "#   第二个参数frame表示截取到一帧的图片\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        if g_frame % (fps * 2) == 0:  #每2秒改变字幕\n",
    "            strInfo = 'abc'+str(g_frame)\n",
    "        frame = cv2.putText(frame, strInfo, (g_frame % 50 ,30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 3)\n",
    "        cv2.rectangle(frame, (0, 0), (200, 40), (255, 0, 0), 3)\n",
    "        frame = cv2.resize(frame, (800, 600))\n",
    "        cv2.imshow('image', frame)\n",
    "        out.write(frame)\n",
    "        g_frame = g_frame + 1\n",
    "    key = cv2.waitKey(1000//fps)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 6.] \n",
      " [3. 6.]\n",
      "加法 8.700001\n",
      "减法 -3.6999998\n",
      "乘法 16.820002\n",
      "除法 2.9\n",
      "WARNING:tensorflow:From <ipython-input-2-0a1648e0f3b5>:35: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "除法 2.9\n",
      "1到100的和： 5050\n",
      "1到10的乘积： 3628800\n",
      "f: [[2. 5. 4.]\n",
      " [1. 3. 6.]]\n",
      "转置： [[2. 1.]\n",
      " [5. 3.]\n",
      " [4. 6.]]\n",
      "叉乘： [[ 4. 10.  8.]\n",
      " [ 2.  6. 12.]]\n",
      "矩阵乘： [[21. 20.]\n",
      " [23. 15.]]\n",
      "求和reduce_sum所有 21.0\n",
      "求和reduce_sum按行 [11. 10.]\n",
      "求和reduce_sum按列 [ 3.  8. 10.]\n",
      "行向量最大值的下标argmax= [1 2]\n",
      "类型转换（布尔到浮点数）cast   1.0\n",
      "cast   0.0\n",
      "比较（真）  True\n",
      "比较（真）  True\n",
      "比较（真）  True\n",
      "所有元素平均值reduce_mean= 3.5\n",
      "列向量平均值reduce_mean(0)= [1.5 4.  5. ]\n",
      "行向量平均值reduce_mean(1)= [3.6666667 3.3333333]\n"
     ]
    }
   ],
   "source": [
    "#tansorflow基本语法\n",
    "import tensorflow as tf\n",
    "\n",
    "c1 = tf.constant(9.5,dtype=tf.float32)#常量\n",
    "c2 = tf.constant(10,dtype=tf.int32)#常量\n",
    "a = tf.Variable(5.8)#变量\n",
    "b = tf.Variable(2.9)\n",
    "sum = tf.Variable(0, name=\"sum\")\n",
    "result = tf.Variable(1, name=\"result\")\n",
    "\n",
    "f = tf.Variable([[2., 5., 4.],\n",
    "                 [1., 3., 6.]])\n",
    "f2 = tf.Variable([[2., 3.],\n",
    "                 [1., 2.],\n",
    "                 [3., 1.]])\n",
    "vector1 = tf.constant([3.,3.]) #这里只有一对中括号 []，就是向量\n",
    "vector2 = tf.constant([1.,2.])\n",
    "result3 = tf.multiply(vector1,vector2) #向量乘法。tf.multiply()\n",
    "result4 = tf.multiply(vector2,vector1)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "r3 = sess.run(result3)1\n",
    "r4 = sess.run(result4)\n",
    "print(r3,'\\n',r4)\n",
    "\n",
    "#tf.add 加法函数\n",
    "print(\"加法\", sess.run(tf.add(a,b))) # 求和\n",
    "#tf.subtract 减法函数\n",
    "print(\"减法\", sess.run(tf.subtract(a,c1))) #减法\n",
    "#tf.multiply 乘法函数\n",
    "print(\"乘法\",sess.run(tf.multiply(a,b))) # 乘积\n",
    "#tf.divde   除法函数\n",
    "print(\"除法\",sess.run(tf.divide(a,2.0))) #除法\n",
    "print(\"除法\",sess.run(tf.div(a,2.0))) #除法\n",
    "\n",
    "#tf.assign 赋值函数\n",
    "for i in range(101):\n",
    "    sess.run(tf.assign(sum, tf.add(sum, i)))\n",
    "print('1到100的和：', sess.run(sum))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    sess.run(tf.assign(result, tf.multiply(result, i)))\n",
    "print('1到10的乘积：', sess.run(result))\n",
    "\n",
    "print(\"f:\" ,sess.run(f))\n",
    "print(\"转置：\", sess.run(tf.transpose(f))) #矩阵转置\n",
    "print(\"叉乘：\", sess.run(tf.multiply(f, 2))) #叉乘\n",
    "print(\"矩阵乘：\", sess.run(tf.matmul(f,f2))) #矩阵乘法\n",
    "print(\"求和reduce_sum所有\", sess.run(tf.reduce_sum(f)))\n",
    "print(\"求和reduce_sum按行\", sess.run(tf.reduce_sum(f, axis=1)))\n",
    "print(\"求和reduce_sum按列\", sess.run(tf.reduce_sum(f, axis=0)))\n",
    "\n",
    "print('行向量最大值的下标argmax=', sess.run(tf.argmax(f, axis=1)))             #行向量最大值的下标\n",
    "print('类型转换（布尔到浮点数）cast  ',sess.run(tf.cast(1 > 0.5, dtype=tf.float32)))  #类型转换\n",
    "print('cast  ',sess.run(tf.cast(0.1 > 0.5, dtype=tf.float32)))  #类型转换\n",
    "print('比较（真） ', sess.run(tf.equal(1.0,1))) #比较\n",
    "print('比较（真） ', sess.run(tf.equal(1.0,1.0))) #比较\n",
    "print('比较（真） ', sess.run(tf.equal(0, False))) #比较\n",
    "print('所有元素平均值reduce_mean=', sess.run(tf.reduce_mean(f)))      #所有元素平均值\n",
    "print('列向量平均值reduce_mean(0)=', sess.run(tf.reduce_mean(f,0))) #列向量平均值\n",
    "print('行向量平均值reduce_mean(1)=', sess.run(tf.reduce_mean(f,1))) #行向量平均值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.180808 [-0.8267349] [0.9212043]\n",
      "100 0.18602546 [0.500259] [1.1360034]\n",
      "200 0.114952475 [0.60716534] [0.8930056]\n",
      "300 0.07103366 [0.6911961] [0.70198387]\n",
      "400 0.043894514 [0.75725204] [0.5518233]\n",
      "500 0.027124127 [0.809178] [0.4337834]\n",
      "600 0.016761074 [0.8499964] [0.34099331]\n",
      "700 0.010357319 [0.8820836] [0.26805177]\n",
      "800 0.0064002 [0.90730685] [0.21071316]\n",
      "900 0.0039549256 [0.9271349] [0.16563964]\n",
      "1000 0.002443909 [0.9427213] [0.13020787]\n",
      "1100 0.001510183 [0.95497376] [0.10235526]\n",
      "1200 0.00093320565 [0.9646052] [0.08046055]\n",
      "1300 0.0005766605 [0.97217655] [0.06324922]\n",
      "1400 0.00035633927 [0.97812825] [0.0497196]\n",
      "1500 0.00022019661 [0.98280686] [0.03908405]\n",
      "1600 0.00013606754 [0.9864846] [0.03072362]\n",
      "1700 8.408244e-05 [0.9893757] [0.02415165]\n",
      "1800 5.195801e-05 [0.9916483] [0.01898536]\n",
      "1900 3.2106993e-05 [0.99343485] [0.01492417]\n",
      "2000 1.9840192e-05 [0.9948392] [0.01173179]\n",
      "[4.9859276]\n",
      "[1.5039905 3.493669 ]\n"
     ]
    }
   ],
   "source": [
    "# 单变量线性回归\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777) #设置随机种子\n",
    "#定义数据集\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "#定义占位符\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "#权重和偏置\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "#预测模型\n",
    "hypothesis = X * W + b\n",
    "#代价或损失函数\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "#梯度下降优化器\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "#创建会话\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) #全局变量初始化\n",
    "#迭代训练\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:# 显示损失值收敛情况\n",
    "        print(step, cost_val, W_val, b_val)\n",
    "#验证\n",
    "print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 22655.951\n",
      "100 5.6471696\n",
      "200 5.449226\n",
      "300 5.2612543\n",
      "400 5.082733\n",
      "500 4.9132347\n",
      "600 4.752207\n",
      "700 4.5992475\n",
      "800 4.453902\n",
      "900 4.315825\n",
      "1000 4.1846113\n",
      "1100 4.0598507\n",
      "1200 3.9412675\n",
      "1300 3.8285565\n",
      "1400 3.7213066\n",
      "1500 3.6193376\n",
      "1600 3.5223286\n",
      "1700 3.4300094\n",
      "1800 3.3421352\n",
      "1900 3.2584965\n",
      "2000 3.1788766\n",
      "[[10.545852]]\n"
     ]
    }
   ],
   "source": [
    "#多变量线性回归\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777) #设置随机种子\n",
    "#定义数据集\n",
    "x_data = [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]]\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]\n",
    "#定义占位符\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "#权重和偏置\n",
    "W = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "#预测模型\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "#代价或损失函数\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "#梯度下降优化器\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(cost)\n",
    "#创建会话\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) #全局变量初始化\n",
    "#迭代训练\n",
    "for step in range(2001):\n",
    "    cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:# 显示损失值收敛情况\n",
    "        print(step, cost_val)\n",
    "#验证或预测\n",
    "print(sess.run(hypothesis, feed_dict={X: [[5,6,7]]}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0649921\n",
      "500 0.57622576\n",
      "1000 0.51355666\n",
      "1500 0.46409324\n",
      "2000 0.42329922\n",
      "2500 0.38843378\n",
      "3000 0.35808048\n",
      "3500 0.33143985\n",
      "4000 0.30795875\n",
      "4500 0.28719458\n",
      "5000 0.26877287\n"
     ]
    }
   ],
   "source": [
    "#逻辑回归\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777) #设置随机种子\n",
    "#定义数据集\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "#定义占位符\n",
    "X = tf.placeholder(\"float\", shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "#权重和偏置\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "#预测模型\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "#代价或损失函数\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "#梯度下降优化器\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "#准确率计算\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "#创建会话\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) #全局变量初始化\n",
    "#迭代训练\n",
    "for step in range(5001):\n",
    "    cost_val, acc, _ = sess.run([cost, accuracy, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 500 == 0:\n",
    "        print(step,cost_val)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.132946 0.375\n",
      "500 0.7399779 0.625\n",
      "1000 0.59210396 0.625\n",
      "1500 0.5266635 0.75\n",
      "2000 0.48397183 0.75\n",
      "2500 0.45239753 0.75\n",
      "3000 0.42733666 0.875\n",
      "3500 0.40646008 0.875\n",
      "4000 0.38845515 0.875\n",
      "4500 0.37253207 0.875\n",
      "5000 0.35819232 0.875\n",
      "\n",
      "Hypothesis:  [[1.6337873e-03 2.7909387e-02 9.7045678e-01]\n",
      " [4.9290881e-03 1.4813985e-01 8.4693104e-01]\n",
      " [8.5663231e-04 3.6721703e-01 6.3192642e-01]\n",
      " [1.0422050e-03 6.7834753e-01 3.2061026e-01]\n",
      " [5.0170225e-01 4.6867484e-01 2.9622918e-02]\n",
      " [2.5831711e-01 7.4083734e-01 8.4556703e-04]\n",
      " [6.1279231e-01 3.8562122e-01 1.5864613e-03]\n",
      " [7.5990069e-01 2.3992004e-01 1.7927105e-04]] \n",
      "Correct (Y):  [2 2 2 1 0 1 0 0] \n",
      "Accuracy:  0.875\n",
      "[[0.0970411  0.8234568  0.07950209]] [1]\n",
      "[[2.3510556e-06 1.1446718e-02 9.8855096e-01]\n",
      " [3.5298620e-03 7.5918698e-01 2.3728316e-01]] \n",
      " [2 1]\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "#softmax多分类\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777) #设置随机种子\n",
    "#定义数据集\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "#定义占位符\n",
    "X = tf.placeholder(\"float\", shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "#权重和偏置\n",
    "W = tf.Variable(tf.random_normal([4, 3]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([3]), name='bias')\n",
    "#预测模型\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "#代价或损失函数\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "#梯度下降优化器\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "#准确率计算\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(y_data, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#创建会话\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) #全局变量初始化\n",
    "#迭代训练\n",
    "for step in range(5001):\n",
    "    cost_val, _, acc = sess.run([cost, train, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 500 == 0:# 显示损失值收敛情况\n",
    "        print(step, cost_val, acc)\n",
    "#准确率\n",
    "h, c, a = sess.run([hypothesis, prediction, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n",
    "#测试\n",
    "h1, p1 = sess.run([hypothesis, prediction], feed_dict={X: [[1, 2, 3, 4]]})\n",
    "print(h1, p1)\n",
    "h2, p2 = sess.run([hypothesis, prediction], feed_dict={X: [[4,1,2,3], [3,2,4,5]]})\n",
    "print(h2, '\\n', p2)\n",
    "# 以下代码临时测试用\n",
    "while True:\n",
    "    str = input()\n",
    "    try:\n",
    "        if str == 'exit':\n",
    "            break\n",
    "        test = list(map(float,str.split(',')))\n",
    "        print(test)\n",
    "        h1, p1 = sess.run([hypothesis, prediction], feed_dict={X: [test]})\n",
    "        print(h1, p1)\n",
    "    except:\n",
    "        continue\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-aa3467389ae4>:38: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0 9.394003 0.0990099\n",
      "100 0.5465805 0.8811881\n",
      "200 0.3679589 0.9108911\n",
      "300 0.2814471 0.9108911\n",
      "400 0.2271465 0.9306931\n",
      "500 0.19008876 0.9306931\n",
      "600 0.16344424 0.97029704\n",
      "700 0.14344649 0.97029704\n",
      "800 0.1278924 0.980198\n",
      "900 0.11543997 0.980198\n",
      "1000 0.10523592 0.990099\n",
      "1100 0.096714854 0.990099\n",
      "1200 0.08948754 0.990099\n",
      "1300 0.08327724 1.0\n",
      "1400 0.0778817 1.0\n",
      "1500 0.07314934 1.0\n",
      "\n",
      "Hypothesis:  [[ 9.706674   -1.8512894   2.1021788  -2.811112    3.3081298   1.0233091\n",
      "   1.9285188 ]\n",
      " [10.652494    0.2490932   4.5209336  -0.1562761   2.740791    1.7095212\n",
      "   1.0560975 ]\n",
      " [ 0.17070913  0.6821901   3.3631935   6.8986025   2.9814847  -3.3284426\n",
      "   1.3431354 ]\n",
      " [ 9.706674   -1.8512894   2.1021788  -2.811112    3.3081298   1.0233091\n",
      "   1.9285188 ]] \n",
      "Correct (Y):  [0 0 3 0 0 0 0 3 3 0 0 1 3 6 6 6 1 0 3 0 1 1 0 1 5 4 4 0 0 0 5 0 0 1 3 0 0\n",
      " 1 3 5 5 1 5 1 0 0 6 0 0 0 0 5 4 6 0 0 1 1 1 1 3 3 2 0 0 0 0 0 0 0 0 1 6 3\n",
      " 0 0 2 6 1 1 2 6 3 1 0 6 3 1 5 4 2 2 3 0 0 1 0 5 0 6 1] \n",
      "Accuracy:  1.0\n",
      "[[12.304025   -0.22565287  5.9762034  -3.853352    6.22513     6.9402556\n",
      "   3.808949  ]] [0]\n"
     ]
    }
   ],
   "source": [
    "# softmax多分类另一种方式lab-06-2-softmax_zoo_classifier.py代码\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777) #设置随机种子\n",
    "#定义数据集\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',')\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "#定义占位符\n",
    "X = tf.placeholder(\"float\", shape=[None, 16])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])  # 0 ~ 6\n",
    "Y_one_hot = tf.one_hot(tf.cast(Y,tf.int32), nb_classes)  # one hot\n",
    "'''\n",
    "[\n",
    "    [[1,0,0,0,0,0,0]]\n",
    "    [[0,1,0,0,0,0,0]]\n",
    "]\n",
    "'''\n",
    "# print(Y_one_hot.shape)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "'''\n",
    "[\n",
    "    [1,0,0,0,0,0,0]\n",
    "    [0,1,0,0,0,0,0]\n",
    "]\n",
    "'''\n",
    "# print(Y_one_hot.shape)\n",
    "#权重和偏置\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "#预测模型\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = logits #预测模型之一，只用于softmax_cross_entropy_with_logits\n",
    "# hypothesis = tf.nn.softmax(logits)  #预测模型之二，都可用\n",
    "#代价或损失函数\n",
    "# cost = tf.reduce_mean(-tf.reduce_sum(Y_one_hot * tf.log(hypothesis), axis=1))\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot))\n",
    "#梯度下降优化器\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "#准确率计算\n",
    "prediction = tf.argmax(logits, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#创建会话\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) #全局变量初始化\n",
    "#迭代训练\n",
    "for step in range(1501):\n",
    "    cost_val, _, acc = sess.run([cost, train, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:# 显示损失值收敛情况\n",
    "        print(step, cost_val, acc)\n",
    "#准确率\n",
    "h, c, a = sess.run([hypothesis, prediction, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "print(\"\\nHypothesis: \", h[:4], \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n",
    "# 测试\n",
    "h1, p1 = sess.run([hypothesis, prediction],\n",
    "    feed_dict={X: [[1,0,0,1,0,0,0,1,1,1,0,0,8,1,0,1]]})\n",
    "print(h1, p1)\n",
    "# while True:\n",
    "#     str = input()\n",
    "#     try:\n",
    "#         if str == 'q':\n",
    "#             break\n",
    "#         test = list(map(float,str.split(',')))\n",
    "#         h1, p1 = sess.run([hypothesis, prediction], feed_dict={X: [test]})\n",
    "#         print(h1, p1)\n",
    "#     except:\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.67584896 0.5\n",
      "100 Cost:  0.6616589 0.5\n",
      "200 Cost:  0.64544713 0.5\n",
      "300 Cost:  0.629091 0.75\n",
      "400 Cost:  0.6121032 0.75\n",
      "500 Cost:  0.59335506 0.75\n",
      "600 Cost:  0.5719359 0.75\n",
      "700 Cost:  0.5467502 0.75\n",
      "800 Cost:  0.51654214 0.75\n",
      "900 Cost:  0.4804983 0.75\n",
      "1000 Cost:  0.43885103 0.75\n",
      "1100 Cost:  0.39324242 1.0\n",
      "1200 Cost:  0.34657484 1.0\n",
      "1300 Cost:  0.30200356 1.0\n",
      "1400 Cost:  0.2618183 1.0\n",
      "1500 Cost:  0.22704536 1.0\n",
      "1600 Cost:  0.19770466 1.0\n",
      "1700 Cost:  0.1732612 1.0\n",
      "1800 Cost:  0.15297988 1.0\n",
      "1900 Cost:  0.13612604 1.0\n",
      "2000 Cost:  0.12205249 1.0\n",
      "2100 Cost:  0.11022183 1.0\n",
      "2200 Cost:  0.10020143 1.0\n",
      "2300 Cost:  0.09164788 1.0\n",
      "2400 Cost:  0.084290035 1.0\n",
      "2500 Cost:  0.07791372 1.0\n",
      "2600 Cost:  0.07234901 1.0\n",
      "2700 Cost:  0.06746021 1.0\n",
      "2800 Cost:  0.06313893 1.0\n",
      "2900 Cost:  0.059297338 1.0\n",
      "3000 Cost:  0.05586377 1.0\n",
      "\n",
      "Hypothesis: \n",
      " [[0.03749581]\n",
      " [0.9336221 ]\n",
      " [0.9539218 ]\n",
      " [0.06690764]] \n",
      "Correct: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy: \n",
      " 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJAip3EVEgpDKXUQkCP0fC3ZqzGFVMkEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dc332d22e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 异或神经网络BP\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)  #设置随机种子\n",
    "learning_rate = 0.1\n",
    "# 定义数据集\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "#定义占位符\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "#模型和前向传播\n",
    "W1 = tf.Variable(tf.random_normal([2, 3]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([3]), name='bias1')\n",
    "a1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "a2 = tf.sigmoid(tf.matmul(a1, W2) + b2)\n",
    "# 代价或损失函数\n",
    "cost = -tf.reduce_mean(Y * tf.log(a2) + (1 - Y) * tf.log(1 - a2))\n",
    "cost_history = [] # 损失值列表\n",
    "#BP反向传播\n",
    "#第2层\n",
    "dz2 = a2 - Y\n",
    "dW2 = tf.matmul(tf.transpose(a1), dz2) / tf.cast(tf.shape(a1)[0], dtype=tf.float32)\n",
    "db2 = tf.reduce_mean(dz2)\n",
    "#第1层\n",
    "da1 = tf.matmul(dz2, tf.transpose(W2))\n",
    "dz1 = da1 * a1 * (1 - a1)\n",
    "dW1 = tf.matmul(tf.transpose(X), dz1) / tf.cast(tf.shape(X)[0], dtype=tf.float32)\n",
    "db1 = tf.reduce_mean(dz1, axis=0)\n",
    "# 参数更新\n",
    "update = [\n",
    "  tf.assign(W2, W2 - learning_rate * dW2),\n",
    "  tf.assign(b2, b2 - learning_rate * db2),\n",
    "  tf.assign(W1, W1 - learning_rate * dW1),\n",
    "  tf.assign(b1, b1 - learning_rate * db1)\n",
    "]\n",
    "# 准确率计算\n",
    "predicted = tf.cast(a2 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "# 创建会话\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) #全局变量初始化\n",
    "    # 迭代训练\n",
    "    for step in range(3001):\n",
    "        _, cost_val, acc_val = sess.run([update, cosat, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:# 显示损失值收敛情况\n",
    "            print(step, \"Cost: \", cost_val, acc_val)\n",
    "            cost_history.append(cost_val)\n",
    "    h, c, a = sess.run([a2, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \\n\", h, \"\\nCorrect: \\n\", c, \"\\nAccuracy: \\n\", a)\n",
    "    # 画学习曲线\n",
    "    plt.plot(cost_history[1: len(cost_history)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[0.83494319 0.11482951]\n",
      " [0.66899751 0.46594987]\n",
      " [0.60181666 0.58838408]\n",
      " [0.31836656 0.20502072]\n",
      " [0.87043944 0.02679395]\n",
      " [0.41539811 0.43938369]\n",
      " [0.68635684 0.24833404]\n",
      " [0.97315228 0.68541849]\n",
      " [0.03081617 0.89479913]\n",
      " [0.24665715 0.28584862]\n",
      " [0.31375667 0.47718349]\n",
      " [0.56689254 0.77079148]\n",
      " [0.7321604  0.35828963]\n",
      " [0.15724842 0.94294584]\n",
      " [0.34933722 0.84634483]\n",
      " [0.50304053 0.81299619]\n",
      " [0.23869886 0.9895604 ]\n",
      " [0.4636501  0.32531094]\n",
      " [0.36510487 0.97365522]\n",
      " [0.73350238 0.83833013]\n",
      " [0.61810158 0.12580353]\n",
      " [0.59274817 0.18779828]\n",
      " [0.87150299 0.34679501]\n",
      " [0.25883219 0.50002932]\n",
      " [0.75690948 0.83429824]\n",
      " [0.29316649 0.05646578]\n",
      " [0.10409134 0.88235166]\n",
      " [0.06727785 0.57784761]\n",
      " [0.38492705 0.48384792]\n",
      " [0.69234428 0.19687348]\n",
      " [0.42783492 0.73416985]\n",
      " [0.09696069 0.04883936]]\n",
      "Y_:\n",
      " [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]\n",
      "w1:\n",
      " [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2:\n",
      " [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "After 0 training step(s), loss_mse on all data is 5.13118\n",
      "After 500 training step(s), loss_mse on all data is 0.429111\n",
      "After 1000 training step(s), loss_mse on all data is 0.409789\n",
      "After 1500 training step(s), loss_mse on all data is 0.399923\n",
      "After 2000 training step(s), loss_mse on all data is 0.394146\n",
      "After 2500 training step(s), loss_mse on all data is 0.390597\n",
      "\n",
      "\n",
      "w1:\n",
      " [[-0.7000663   0.9136318   0.08953571]\n",
      " [-2.3402493  -0.14641267  0.58823055]]\n",
      "w2:\n",
      " [[-0.06024267]\n",
      " [ 0.91956186]\n",
      " [-0.0682071 ]]\n"
     ]
    }
   ],
   "source": [
    "#周考代码\n",
    "# coding:utf-8\n",
    "# 0导入模块，生成模拟数据集。\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 8#批处理参数\n",
    "SEED = 23455\n",
    "\n",
    "# 基于seed产生随机数\n",
    "rdm = np.random.RandomState(SEED)\n",
    "# 随机数返回32行2列的矩阵  作为输入数据集\n",
    "X = rdm.rand(32, 2)\n",
    "# 作为输入数据集的标签（正确答案）\n",
    "Y_ = [[int(x0 + x1 < 1)] for (x0, x1) in X]\n",
    "print(\"X:\\n\", X)\n",
    "print(\"Y_:\\n\", Y_)\n",
    "\n",
    "# 1定义神经网络的输入、参数和输出,定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 2定义损失函数及反向传播方法。\n",
    "loss_mse = tf.reduce_mean(tf.square(y - y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "# train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss_mse)\n",
    "# train_step = tf.train.AdamOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "# 3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    # 输出目前（未经训练）的参数取值。\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 训练模型。\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss_mse, feed_dict={x: X, y_: Y_})\n",
    "            print(\"After %d training step(s), loss_mse on all data is %g\" % (i, total_loss))\n",
    "\n",
    "    # 输出训练后的参数取值。\n",
    "    print(\"\\n\")\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
